import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# 数据集
X, y = make_moons(n_samples=2000, noise=0.25, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定义模型
rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, oob_score=True)
gbdt = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)

# 训练模型
rf.fit(X_train, y_train)
gbdt.fit(X_train, y_train)

# 准备绘图网格
xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200),
                     np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 200))
grid = np.c_[xx.ravel(), yy.ravel()]

# 计算决策边界
Z_rf = rf.predict(grid).reshape(xx.shape)
Z_gbdt = gbdt.predict(grid).reshape(xx.shape)

# 1. Decision Boundaries
plt.figure()
plt.contourf(xx, yy, Z_rf, alpha=0.3, cmap='Wistia')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k', cmap='rainbow')
plt.title("RF Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

plt.figure()
plt.contourf(xx, yy, Z_gbdt, alpha=0.3, cmap='viridis')
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k', cmap='rainbow')
plt.title("GBDT Decision Boundary")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

# 2. Accuracy vs Number of Trees
n_trees = list(range(1, 51))
acc_rf = []
acc_gbdt = []
for n in n_trees:
    rf_n = RandomForestClassifier(n_estimators=n, max_depth=5, random_state=42)
    gbdt_n = GradientBoostingClassifier(n_estimators=n, learning_rate=0.1, max_depth=3, random_state=42)
    rf_n.fit(X_train, y_train)
    gbdt_n.fit(X_train, y_train)
    acc_rf.append(accuracy_score(y_test, rf_n.predict(X_test)))
    acc_gbdt.append(accuracy_score(y_test, gbdt_n.predict(X_test)))

plt.figure()
plt.plot(n_trees, acc_rf, label="RF Accuracy", linewidth=2, color='magenta')
plt.plot(n_trees, acc_gbdt, label="GBDT Accuracy", linewidth=2, color='cyan')
plt.title("Accuracy vs Number of Trees")
plt.xlabel("Number of Trees")
plt.ylabel("Accuracy")
plt.legend()

# 3. Feature Importance
fi_rf = rf.feature_importances_
fi_gbdt = gbdt.feature_importances_
features = ['Feature 1', 'Feature 2']

x = np.arange(len(features))
width = 0.35

plt.figure()
plt.bar(x - width/2, fi_rf, width, label='RF', color='orange')
plt.bar(x + width/2, fi_gbdt, width, label='GBDT', color='lime')
plt.xticks(x, features)
plt.title("Feature Importance Comparison")
plt.ylabel("Importance")
plt.legend()

# 4. OOB Error vs Training Devience
# RF OOB error
oob_error = 1 - rf.oob_score_
# GBDT training deviance
test_deviance = np.zeros((50,), dtype=np.float64)
for i, pred in enumerate(gbdt.staged_predict_proba(X_test)):
    test_deviance[i] = gbdt.loss_(y_test, pred[:, 1])

plt.figure()
plt.plot([50], [oob_error], 'o', label='RF OOB Error', markersize=8, color='red')
plt.plot(n_trees, test_deviance, label='GBDT Test Deviance', linewidth=2, color='blue')
plt.title("RF OOB Error vs GBDT Test Deviance")
plt.xlabel("Number of Trees")
plt.ylabel("Error / Deviance")
plt.legend()

plt.show()
